{"cells":[{"cell_type":"markdown","metadata":{},"source":["https://www.youtube.com/watch?v=Wp8YtVPF1cM&list=PLhDAH9aTfnxKfmufxF59vaZECZJD5j6rd&index=16"]},{"cell_type":"markdown","metadata":{},"source":["## 主成分と固有ベクトルの関係\n","\n","主成分分析（PCA：Principal Component Analysis）は、多次元データを低次元に圧縮し、データの特徴を抽出するための手法です。主成分分析の中心にあるのが「主成分」と「固有ベクトル」の関係です。\n","\n","### 主成分分析の基本的な手順\n","\n","1. **データの中心化**：\n","   データセットの各変数の平均を0にします。これは、各データポイントからその平均値を引くことによって行われます。\n","\n","2. **共分散行列の計算**：\n","   中心化したデータの共分散行列を計算します。共分散行列はデータの変数間の共分散を表す対称行列です。\n","\n","3. **固有値と固有ベクトルの計算**：\n","   共分散行列の固有値（eigenvalue）と固有ベクトル（eigenvector）を計算します。固有値と固有ベクトルは、以下の固有方程式を解くことで得られます：\n","   $$\n","   \\mathbf{C} \\mathbf{v} = \\lambda \\mathbf{v}\n","   $$\n","   ここで、$\\mathbf{C}$ は共分散行列、$\\mathbf{v}$ は固有ベクトル、$\\lambda$ は固有値です。\n","\n","4. **主成分の選択**：\n","   固有値が大きい順に固有ベクトルを並べ、その順に対応する固有ベクトルが主成分を表します。主成分は、データの分散を最大化する方向を示します。\n","\n","### 主成分と固有ベクトルの関係\n","\n","- **固有ベクトル**：\n","  固有ベクトルは共分散行列の固有ベクトルであり、データの主成分の方向を示します。各固有ベクトルはデータの次元を表し、主成分空間での新しい基底（軸）を提供します。\n","\n","- **固有値**：\n","  各固有ベクトルに対応する固有値は、その固有ベクトルが示す方向に沿ったデータの分散の大きさを表します。固有値が大きいほど、その方向がデータの分散を多く説明します。\n","\n","### 例：Pythonでの主成分分析\n","\n","以下に、Pythonで主成分分析を実行し、主成分と固有ベクトルの関係を確認する例を示します。\n","\n","\n","\n","このコードでは、次のことを行っています：\n","\n","1. ダミーデータを作成し、標準化します。\n","2. PCAを実行し、主成分（固有ベクトル）とそれに対応する固有値を取得します。\n","\n","### 結果の解釈\n","\n","- **固有ベクトル**（主成分）は、新しい次元（基底）を表し、データの分散を最大化する方向を示します。\n","- **固有値**は、それぞれの主成分がどれだけの分散を説明しているかを示します。"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["固有ベクトル（主成分）：\n","[[-0.1912177   0.73187406 -0.65406128]\n"," [ 0.8866293  -0.15708688 -0.43498529]\n"," [ 0.4210989   0.66308678  0.61886319]]\n","\n","固有値：\n","[1.12813043 1.02962554 0.84224404]\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.decomposition import PCA\n","\n","# ダミーデータの作成\n","np.random.seed(0)\n","data = pd.DataFrame({\n","    '変数1': np.random.rand(100),\n","    '変数2': np.random.rand(100),\n","    '変数3': np.random.rand(100)\n","})\n","\n","# データの標準化\n","data_standardized = (data - data.mean()) / data.std()\n","\n","# 主成分分析の実行\n","pca = PCA()\n","pca.fit(data_standardized)\n","\n","# 固有ベクトル（主成分）と固有値の取得\n","eigenvectors = pca.components_\n","eigenvalues = pca.explained_variance_\n","\n","# 結果の表示\n","print(\"固有ベクトル（主成分）：\")\n","print(eigenvectors)\n","print(\"\\n固有値：\")\n","print(eigenvalues)"]},{"cell_type":"markdown","metadata":{},"source":["## 寄与率について\n","\n","主成分分析（PCA）における寄与率（contribution rate, explained variance ratio）は、各主成分が全体のデータ分散に対してどれだけ寄与しているかを示す指標です。これは、主成分分析の結果として得られる固有値を用いて計算されます。\n","\n","### 寄与率の計算\n","\n","各主成分の寄与率は、その主成分に対応する固有値を全固有値の合計で割ったものです。具体的には、次の式で表されます：\n","\n","$$\n","\\text{寄与率} = \\frac{\\lambda_i}{\\sum_{j=1}^{p} \\lambda_j}\n","$$\n","\n","ここで、\n","- $\\lambda_i$ は第 $i$ 主成分に対応する固有値\n","- $p$ は全主成分の数\n","\n","### 累積寄与率\n","\n","累積寄与率は、複数の主成分が全体のデータ分散に対してどれだけ寄与しているかを累積して示す指標です。第 $k$ 主成分までの累積寄与率は次の式で計算されます：\n","\n","$$\n","\\text{累積寄与率} = \\sum_{i=1}^{k} \\frac{\\lambda_i}{\\sum_{j=1}^{p} \\lambda_j}\n","$$\n","\n","### Pythonでの寄与率の計算\n","\n","以下に、Pythonを用いて主成分分析を実行し、寄与率および累積寄与率を計算する例を示します。\n","\n","\n","\n","### 結果の解釈\n","\n","実行すると、次のような出力が得られます：\n","\n","```\n","固有値：\n","[1.67638005 1.0113335  0.31228645]\n","\n","寄与率：\n","[0.55879335 0.3364445  0.10476215]\n","\n","累積寄与率：\n","[0.55879335 0.89523785 1.        ]\n","```\n","\n","この結果から、次のことが分かります：\n","\n","1. 最初の主成分は全体の分散の約55.88%を説明しています。\n","2. 二番目の主成分は全体の分散の約33.64%を説明しています。\n","3. 三番目の主成分は全体の分散の約10.48%を説明しています。\n","4. 最初の二つの主成分だけで全体の分散の約89.52%を説明できることが分かります。\n","\n","このように、寄与率と累積寄与率を用いることで、どの主成分がデータの分散に対してどれだけ重要かを評価し、次元削減の判断材料とすることができます。"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["固有値：\n","[1.12813043 1.02962554 0.84224404]\n","\n","寄与率：\n","[0.37604348 0.34320851 0.28074801]\n","\n","累積寄与率：\n","[0.37604348 0.71925199 1.        ]\n"]}],"source":["import numpy as np\n","import pandas as pd\n","from sklearn.decomposition import PCA\n","\n","# ダミーデータの作成\n","np.random.seed(0)\n","data = pd.DataFrame({\n","    '変数1': np.random.rand(100),\n","    '変数2': np.random.rand(100),\n","    '変数3': np.random.rand(100)\n","})\n","\n","# データの標準化\n","data_standardized = (data - data.mean()) / data.std()\n","\n","# 主成分分析の実行\n","pca = PCA()\n","pca.fit(data_standardized)\n","\n","# 固有値の取得\n","eigenvalues = pca.explained_variance_\n","\n","# 寄与率の計算\n","explained_variance_ratio = pca.explained_variance_ratio_\n","\n","# 累積寄与率の計算\n","cumulative_explained_variance_ratio = np.cumsum(explained_variance_ratio)\n","\n","# 結果の表示\n","print(\"固有値：\")\n","print(eigenvalues)\n","print(\"\\n寄与率：\")\n","print(explained_variance_ratio)\n","print(\"\\n累積寄与率：\")\n","print(cumulative_explained_variance_ratio)"]},{"cell_type":"markdown","metadata":{},"source":["## スペクトル分解と特異値分解について\n","\n","スペクトル分解（Spectral Decomposition）と特異値分解（Singular Value Decomposition, SVD）は、行列を分解するための非常に重要な手法です。それぞれの分解方法について詳しく説明します。\n","\n","### スペクトル分解\n","\n","スペクトル分解は、特に正方行列の固有値分解（eigenvalue decomposition）としても知られています。これは、対称行列を固有値と固有ベクトルに分解する方法です。\n","\n","#### 対称行列のスペクトル分解\n","\n","任意の対称行列 $\\mathbf{A}$ は、次のように分解できます：\n","\n","$$\n","\\mathbf{A} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^T\n","$$\n","\n","ここで、\n","- $\\mathbf{A}$ は $n \\times n$ の対称行列\n","- $\\mathbf{Q}$ は $\\mathbf{A}$ の固有ベクトルを列に持つ正規直交行列\n","- $\\mathbf{\\Lambda}$ は $\\mathbf{A}$ の固有値を対角成分に持つ対角行列\n","\n","スペクトル分解を利用することで、行列の対角化や各種行列関数の計算が容易になります。\n","\n","### 特異値分解\n","\n","特異値分解（SVD）は、任意の行列を特異値と特異ベクトルに分解する方法です。これは、特に非正方行列にも適用可能であり、データ解析や機械学習で広く使用されています。\n","\n","#### 任意行列の特異値分解\n","\n","任意の $m \\times n$ 行列 $\\mathbf{A}$ は、次のように分解できます：\n","\n","$$\n","\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^T\n","$$\n","\n","ここで、\n","- $\\mathbf{U}$ は $m \\times m$ の直交行列（左特異ベクトル）\n","- $\\mathbf{\\Sigma}$ は $m \\times n$ の対角行列（特異値を対角成分に持つ）\n","- $\\mathbf{V}$ は $n \\times n$ の直交行列（右特異ベクトル）\n","\n","特異値分解は、データの次元削減（例えば、主成分分析）、行列の近似、ノイズ除去などに利用されます。\n","\n","### Pythonでの実装例\n","\n","以下に、Pythonを使用してスペクトル分解と特異値分解を実行する例を示します。\n","\n","\n","\n","### 結果の解釈\n","\n","- **スペクトル分解**では、行列 $A$ の固有値と固有ベクトルを取得します。固有値は対角行列 $\\Lambda$ の対角成分であり、固有ベクトルは行列 $\\mathbf{Q}$ の列です。\n","  \n","- **特異値分解**では、行列 $A$ を特異値と特異ベクトルに分解します。$U$ は左特異ベクトル、$\\Sigma$ は特異値を含む対角行列、$V^T$ は右特異ベクトルです。\n","\n","このように、スペクトル分解と特異値分解は、行列の特性を解析する強力なツールであり、さまざまな応用が可能です。"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["固有値：\n","[2.38196601 4.61803399]\n","\n","固有ベクトル：\n","[[ 0.52573111 -0.85065081]\n"," [-0.85065081 -0.52573111]]\n","U行列：\n","[[-0.2298477   0.88346102  0.40824829]\n"," [-0.52474482  0.24078249 -0.81649658]\n"," [-0.81964194 -0.40189603  0.40824829]]\n","\n","特異値：\n","[9.52551809 0.51430058]\n","\n","V^T行列：\n","[[-0.61962948 -0.78489445]\n"," [-0.78489445  0.61962948]]\n"]}],"source":["\n","import numpy as np\n","\n","# 対称行列の定義\n","A = np.array([[4, 1], [1, 3]])\n","\n","# 固有値と固有ベクトルの計算\n","eigenvalues, eigenvectors = np.linalg.eigh(A)\n","\n","# 結果の表示\n","print(\"固有値：\")\n","print(eigenvalues)\n","print(\"\\n固有ベクトル：\")\n","print(eigenvectors)\n","\n","#### 特異値分解\n","\n","import numpy as np\n","\n","# 任意の行列の定義\n","A = np.array([[1, 2], [3, 4], [5, 6]])\n","\n","# 特異値分解の実行\n","U, Sigma, Vt = np.linalg.svd(A)\n","\n","# 結果の表示\n","print(\"U行列：\")\n","print(U)\n","print(\"\\n特異値：\")\n","print(Sigma)\n","print(\"\\nV^T行列：\")\n","print(Vt)"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
