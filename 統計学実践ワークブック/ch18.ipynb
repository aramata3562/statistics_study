{"cells":[{"cell_type":"markdown","metadata":{},"source":["## ロジスティック回帰モデルについて\n","\n","ロジスティック回帰は、回帰分析の一種であり、目的変数がカテゴリカルデータ（通常は二値データ、つまり0または1）である場合に使用される統計モデルです。線形回帰とは異なり、ロジスティック回帰は出力が確率（0から1の範囲）となるように設計されています。\n","\n","### ロジスティック回帰の基本概念\n","\n","ロジスティック回帰モデルは、次の形式で表されます。\n","\n","$$\n","p(\\mathbf{x}) = \\frac{1}{1 + \\exp(-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p))}\n","$$\n","\n","ここで、\n","- $p(\\mathbf{x})$ は、事象が起こる確率（目的変数が1である確率）です。\n","- $\\beta_0$ は切片（バイアス項）です。\n","- $\\beta_1, \\beta_2, \\ldots, \\beta_p$ は回帰係数です。\n","- $x_1, x_2, \\ldots, x_p$ は説明変数です。\n","\n","### ロジスティック関数（シグモイド関数）\n","\n","ロジスティック回帰モデルでは、ロジスティック関数（シグモイド関数）を用いて、線形結合を確率に変換します。ロジスティック関数は以下のように定義されます。\n","\n","$$\n","\\sigma(z) = \\frac{1}{1 + \\exp(-z)}\n","$$\n","\n","ここで、$z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p$ です。\n","\n","### ロジスティック回帰の適用範囲\n","\n","ロジスティック回帰は、次のような二値分類問題に広く適用されます。\n","- スパムメールの判定（スパム or 非スパム）\n","- 疾患の有無の予測（陽性 or 陰性）\n","- 顧客の購買行動の予測（購入する or しない）\n","\n","### パラメータの推定\n","\n","ロジスティック回帰モデルのパラメータ（$\\beta$）は、対数尤度関数を最大化することで推定されます。対数尤度関数は以下のように定義されます。\n","\n","$$\n","\\mathcal{L}(\\beta) = \\sum_{i=1}^{n} \\left( y_i \\log(p(\\mathbf{x}_i)) + (1 - y_i) \\log(1 - p(\\mathbf{x}_i)) \\right)\n","$$\n","\n","ここで、$y_i$ は実際の目的変数の値、$p(\\mathbf{x}_i)$ はモデルが予測する確率です。"]},{"cell_type":"markdown","metadata":{},"source":["## オッズについて\n","\n","オッズ（odds）とは、ある事象が発生する確率と発生しない確率の比率を表す指標です。オッズは確率と密接に関連していますが、確率とは異なる方法で表現されます。\n","\n","### オッズの計算\n","\n","オッズは次のように計算されます：\n","\n","$$\n","\\text{オッズ} = \\frac{\\text{事象が発生する確率}}{\\text{事象が発生しない確率}} = \\frac{p}{1-p}\n","$$\n","\n","ここで、$p$ は事象が発生する確率です。\n","\n","例えば、事象が発生する確率が $0.75$（75%）である場合、オッズは次のようになります：\n","\n","$$\n","\\text{オッズ} = \\frac{0.75}{1-0.75} = \\frac{0.75}{0.25} = 3\n","$$\n","\n","これは、事象が発生する確率が発生しない確率の3倍であることを意味します。\n","\n","### オッズ比（Odds Ratio）\n","\n","オッズ比は、二つのオッズの比率を表す指標で、特にロジスティック回帰分析や疫学研究で重要です。オッズ比は次のように計算されます：\n","\n","$$\n","\\text{オッズ比} = \\frac{\\text{グループAのオッズ}}{\\text{グループBのオッズ}}\n","$$\n","\n","例えば、ある治療群（グループA）と対照群（グループB）のそれぞれのオッズが異なる場合、その効果をオッズ比で評価します。\n","\n","### ロジスティック回帰におけるオッズとオッズ比\n","\n","ロジスティック回帰では、モデルの係数（$\\beta$）がオッズ比に関連しています。具体的には、説明変数 $x_i$ が1単位増加したときのオッズ比は $\\exp(\\beta_i)$ で計算されます。これにより、各説明変数の影響をオッズ比として解釈できます。\n","\n","### オッズの実用例\n","\n","1. **ギャンブル**:\n","    オッズは、スポーツベッティングやカジノゲームなどで、特定の結果が発生する可能性を評価するために使用されます。\n","\n","2. **医療研究**:\n","    疫学研究では、ある治療法の効果をオッズ比を用いて比較します。例えば、新薬が病気を予防する確率とプラセボの確率を比較する際にオッズ比が用いられます。\n","\n","### Pythonコード例\n","\n","以下に、オッズとオッズ比を計算するPythonコードの例を示します。\n","\n","\n","このコードでは、Irisデータセットを使用して2クラスのロジスティック回帰モデルを訓練し、各特徴量のオッズ比を計算して表示します。オッズ比は各特徴量の影響力を解釈するのに役立ちます。"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["sepal length (cm): オッズ比 = 1.5513\n","sepal width (cm): オッズ比 = 0.4031\n","petal length (cm): オッズ比 = 10.0679\n","petal width (cm): オッズ比 = 2.6121\n"]}],"source":["import numpy as np\n","import statsmodels.api as sm\n","from sklearn.datasets import load_iris\n","from sklearn.linear_model import LogisticRegression\n","\n","# Irisデータセットの読み込み（2クラスに限定）\n","iris = load_iris()\n","X = iris.data[iris.target != 2]\n","y = iris.target[iris.target != 2]\n","\n","# ロジスティック回帰モデルの作成と訓練\n","model = LogisticRegression()\n","model.fit(X, y)\n","\n","# 各特徴量のオッズ比の計算\n","odds_ratios = np.exp(model.coef_[0])\n","\n","# オッズ比の表示\n","feature_names = iris.feature_names\n","for feature, odds_ratio in zip(feature_names, odds_ratios):\n","    print(f\"{feature}: オッズ比 = {odds_ratio:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["## オッズと分類の寄与度\n","\n","はい、その通りです。オッズ比は各特徴量が分類にどれだけ寄与するかを示す指標であり、オッズ比が1よりも大きく（または小さく）離れているほど、その特徴量の寄与度が高いことを意味します。\n","\n","### オッズ比の解釈\n","\n","- **オッズ比が1より大きい場合**:\n","    その特徴量が増加すると、事象が発生する確率が増加します。つまり、その特徴量が事象の発生に対して正の寄与をしていることを意味します。\n","\n","- **オッズ比が1より小さい場合**:\n","    その特徴量が増加すると、事象が発生する確率が減少します。つまり、その特徴量が事象の発生に対して負の寄与をしていることを意味します。\n","\n","- **オッズ比が1の場合**:\n","    その特徴量が事象の発生に影響を与えないことを意味します。すなわち、特徴量が変化しても事象が発生する確率には変化がないということです。\n","\n","### オッズ比の例\n","\n","具体的な例を考えてみましょう。\n","\n","### 例1: オッズ比が1より大きい場合\n","\n","- **オッズ比 = 2**:\n","    例えば、ある特徴量のオッズ比が2である場合、その特徴量が1単位増加すると、事象が発生するオッズが2倍になります。これは、事象が発生する確率が増加することを意味します。\n","\n","### 例2: オッズ比が1より小さい場合\n","\n","- **オッズ比 = 0.5**:\n","    例えば、ある特徴量のオッズ比が0.5である場合、その特徴量が1単位増加すると、事象が発生するオッズが半分になります。これは、事象が発生する確率が減少することを意味します。\n","\n","例えば、以下のような結果が得られたとします：\n","\n","- **sepal length**: オッズ比 = 1.5\n","- **sepal width**: オッズ比 = 0.8\n","- **petal length**: オッズ比 = 2.3\n","- **petal width**: オッズ比 = 0.5\n","\n","この場合の解釈は次の通りです：\n","\n","- **sepal length**:\n","    - オッズ比が1.5であるため、sepal lengthが1単位増加すると、Setosaであるオッズが1.5倍になります。\n","    - 事象が発生する確率（ここではSetosaである確率）が増加することを示します。\n","\n","- **sepal width**:\n","    - オッズ比が0.8であるため、sepal widthが1単位増加すると、Setosaであるオッズが0.8倍になります。\n","    - 事象が発生する確率が減少することを示します。\n","\n","- **petal length**:\n","    - オッズ比が2.3であるため、petal lengthが1単位増加すると、Setosaであるオッズが2.3倍になります。\n","    - 事象が発生する確率が大きく増加することを示します。\n","\n","- **petal width**:\n","    - オッズ比が0.5であるため、petal widthが1単位増加すると、Setosaであるオッズが0.5倍になります。\n","    - 事象が発生する確率が減少することを示します。\n","\n","オッズ比が1から離れている特徴量ほど、分類の寄与度が高いと言えます。"]},{"cell_type":"markdown","metadata":{},"source":["## プロビットモデルについて\n","\n","プロビットモデルは、ロジスティック回帰と同様に、カテゴリカルデータ（特に二値データ）を扱うための回帰モデルです。ロジスティック回帰ではシグモイド関数を使用して確率をモデル化しますが、プロビットモデルでは標準正規分布の累積分布関数（CDF）を使用します。\n","\n","### プロビットモデルの基本概念\n","\n","プロビットモデルは次の形式で表されます：\n","\n","$$\n","P(Y = 1 | \\mathbf{x}) = \\Phi(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p)\n","$$\n","\n","ここで、\n","- $P(Y = 1 | \\mathbf{x})$ は、目的変数 $Y$ が1になる確率です。\n","- $\\Phi$ は標準正規分布の累積分布関数（CDF）です。\n","- $\\beta_0$ は切片（バイアス項）です。\n","- $\\beta_1, \\beta_2, \\ldots, \\beta_p$ は回帰係数です。\n","- $x_1, x_2, \\ldots, x_p$ は説明変数です。\n","\n","### 標準正規分布の累積分布関数\n","\n","標準正規分布の累積分布関数（CDF）は以下のように定義されます：\n","\n","$$\n","\\Phi(z) = \\int_{-\\infty}^{z} \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{t^2}{2}} dt\n","$$\n","\n","この関数は、ある値 $z$ 以下の確率を与えます。プロビットモデルでは、このCDFを用いて線形結合を確率に変換します。\n","\n","### プロビットモデルの適用範囲\n","\n","プロビットモデルは、以下のような二値分類問題に適用されます：\n","- 疾患の有無の予測（陽性 or 陰性）\n","- 顧客の購入意向の予測（購入する or しない）\n","- 投票行動の予測（投票する or しない）\n","\n","### プロビットモデルのパラメータ推定\n","\n","プロビットモデルのパラメータは、尤度関数を最大化することで推定されます。尤度関数は以下のように定義されます：\n","\n","$$\n","\\mathcal{L}(\\beta) = \\prod_{i=1}^{n} [\\Phi(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip})]^{y_i} \\times [1 - \\Phi(\\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\cdots + \\beta_p x_{ip})]^{1 - y_i}\n","$$\n","\n","ここで、$y_i$ は実際の目的変数の値、$\\Phi$ は標準正規分布の累積分布関数です。\n","\n","### Pythonコード例\n","\n","以下に、Pythonを用いたプロビットモデルの実装例を示します。\n","\n","\n","\n","### 結果の解釈\n","\n","プロビットモデルを実行すると、以下の情報が得られます：\n","\n","- **係数**: 各特徴量に対する係数が表示されます。これらの係数は、各特徴量が目的変数に与える影響を示します。係数が大きいほど、その特徴量の影響が大きいことを意味します。\n","\n","- **統計的有意性**: 係数に対するp値が表示され、各特徴量の統計的有意性を評価できます。p値が小さいほど、その特徴量がモデルにとって重要であることを示します。\n","\n","- **モデルの適合度**: ロジスティック回帰と同様に、プロビットモデルの適合度を評価するための指標（例えば、擬似R^2値など）が提供されます。\n","\n","### プロビットモデルの選択\n","\n","ロジスティック回帰とプロビットモデルのどちらを選択するかは、具体的な問題やデータの性質に依存します。一般的には、両者の結果は非常に似ていることが多く、選択は好みによる場合が多いです。ただし、ある特定の理論的背景やデータ分布の特性に基づいて、一方が他方より適している場合もあります。"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Warning: Maximum number of iterations has been exceeded.\n","         Current function value: 0.000000\n","         Iterations: 35\n","                          Probit Regression Results                           \n","==============================================================================\n","Dep. Variable:                      y   No. Observations:                  100\n","Model:                         Probit   Df Residuals:                       95\n","Method:                           MLE   Df Model:                            4\n","Date:                Tue, 11 Jun 2024   Pseudo R-squ.:                   1.000\n","Time:                        20:09:25   Log-Likelihood:            -8.2473e-07\n","converged:                      False   LL-Null:                       -69.315\n","Covariance Type:            nonrobust   LLR p-value:                 5.547e-29\n","==============================================================================\n","                 coef    std err          z      P>|z|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const          2.2563   8902.673      0.000      1.000   -1.74e+04    1.75e+04\n","x1            -2.7374   2506.224     -0.001      0.999   -4914.847    4909.372\n","x2            -1.3371    806.602     -0.002      0.999   -1582.248    1579.574\n","x3             4.7698   1063.458      0.004      0.996   -2079.570    2089.110\n","x4             5.8694   2026.865      0.003      0.998   -3966.712    3978.451\n","==============================================================================\n","\n","Complete Separation: The results show that there iscomplete separation or perfect prediction.\n","In this case the Maximum Likelihood Estimator does not exist and the parameters\n","are not identified.\n","\n","各特徴量の係数:\n","[ 2.2563217  -2.73736702 -1.33707119  4.76977675  5.86940208]\n"]},{"name":"stderr","output_type":"stream","text":["/Users/taketoaramaki/study/statistics_study/.venv/lib/python3.12/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n","  warnings.warn(msg, category=PerfectSeparationWarning)\n","/Users/taketoaramaki/study/statistics_study/.venv/lib/python3.12/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n","  warnings.warn(msg, category=PerfectSeparationWarning)\n","/Users/taketoaramaki/study/statistics_study/.venv/lib/python3.12/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n","  warnings.warn(msg, category=PerfectSeparationWarning)\n","/Users/taketoaramaki/study/statistics_study/.venv/lib/python3.12/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n","  warnings.warn(msg, category=PerfectSeparationWarning)\n","/Users/taketoaramaki/study/statistics_study/.venv/lib/python3.12/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n","  warnings.warn(msg, category=PerfectSeparationWarning)\n","/Users/taketoaramaki/study/statistics_study/.venv/lib/python3.12/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n","  warnings.warn(msg, category=PerfectSeparationWarning)\n","/Users/taketoaramaki/study/statistics_study/.venv/lib/python3.12/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n","  warnings.warn(msg, category=PerfectSeparationWarning)\n","/Users/taketoaramaki/study/statistics_study/.venv/lib/python3.12/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n","  warnings.warn(msg, category=PerfectSeparationWarning)\n","/Users/taketoaramaki/study/statistics_study/.venv/lib/python3.12/site-packages/statsmodels/discrete/discrete_model.py:227: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n","  warnings.warn(msg, category=PerfectSeparationWarning)\n","/Users/taketoaramaki/study/statistics_study/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n","  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"]}],"source":["\n","import numpy as np\n","import pandas as pd\n","import statsmodels.api as sm\n","from sklearn.datasets import load_iris\n","\n","# Irisデータセットの読み込み（2クラスに限定）\n","iris = load_iris()\n","X = iris.data[iris.target != 2]  # SetosaとVersicolorのデータ\n","y = iris.target[iris.target != 2]  # Setosa: 0, Versicolor: 1\n","\n","# 切片項の追加\n","X = sm.add_constant(X)\n","\n","# プロビットモデルの作成とフィッティング\n","probit_model = sm.Probit(y, X).fit()\n","\n","# モデルの要約を表示\n","print(probit_model.summary())\n","\n","# 各特徴量の影響（係数）の表示\n","coefficients = probit_model.params\n","print(\"\\n各特徴量の係数:\")\n","print(coefficients)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## ポアソン回帰モデルについて\n","\n","ポアソン回帰モデルは、カウントデータ（例えば、ある期間内のイベントの発生回数）を扱うための回帰モデルです。このモデルは、目的変数がポアソン分布に従うと仮定します。ポアソン回帰は、交通事故の発生回数や病気の発症回数など、イベントの発生頻度を予測する際に広く使用されます。\n","\n","### ポアソン分布\n","\n","ポアソン分布は、単位時間あたりに一定の平均発生率でイベントが発生する場合の確率分布です。ポアソン分布の確率質量関数は次のように定義されます：\n","\n","$$\n","P(Y = y) = \\frac{\\lambda^y e^{-\\lambda}}{y!}\n","$$\n","\n","ここで、$\\lambda$ は平均発生率（平均発生回数）、$y$ は観測された発生回数です。\n","\n","### ポアソン回帰モデル\n","\n","ポアソン回帰モデルでは、説明変数の線形結合を使ってポアソン分布のパラメータ $\\lambda$ を予測します。モデルは次のように定義されます：\n","\n","$$\n","\\log(\\lambda) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p\n","$$\n","\n","ここで、\n","- $\\lambda$ は予測される発生率です。\n","- $\\beta_0$ は切片（バイアス項）です。\n","- $\\beta_1, \\beta_2, \\ldots, \\beta_p$ は回帰係数です。\n","- $x_1, $x_2, \\ldots, $x_p$ は説明変数です。\n","\n","$\\lambda$ を予測するために対数リンク関数を使用します。このリンク関数により、モデルの出力が常に正の値になることが保証されます。\n","\n","### ポアソン回帰の適用例\n","\n","ポアソン回帰モデルは、次のようなカウントデータを予測するのに適しています：\n","- ある期間内の事故の発生回数\n","- 店舗に訪れる顧客の数\n","- 患者が病院を訪れる回数\n","\n","### パラメータの推定\n","\n","ポアソン回帰モデルのパラメータは、最大尤度推定法（MLE）を用いて推定されます。尤度関数は以下のように定義されます：\n","\n","$$\n","\\mathcal{L}(\\beta) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda_i} \\lambda_i^{y_i}}{y_i!}\n","$$\n","\n","ここで、$\\lambda_i$ は観測 $i$ における予測発生率、$y_i$ は観測された発生回数です。\n","\n","### Pythonコード例\n","\n","以下に、Pythonを用いたポアソン回帰モデルの実装例を示します。\n","\n","### 結果の解釈\n","\n","ポアソン回帰モデルの実行結果には、以下の情報が含まれます：\n","\n","- **係数**: 各特徴量に対する回帰係数が表示されます。これらの係数は、各特徴量がカウントデータに与える影響を示します。\n","- **統計的有意性**: 係数に対するp値が表示され、各特徴量の統計的有意性を評価できます。p値が小さいほど、その特徴量がモデルにとって重要であることを示します。\n","- **モデルの適合度**: ロジスティック回帰やプロビットモデルと同様に、ポアソン回帰モデルの適合度を評価するための指標が提供されます。\n","\n","### ポアソン回帰の選択\n","\n","ポアソン回帰モデルは、特にカウントデータや非負整数値データを扱う場合に適しています。データがポアソン分布に従うと仮定される場合に最適です。ロジスティック回帰やプロビットモデルが二値データに適しているのに対し、ポアソン回帰は頻度データに適しています。"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                 Generalized Linear Model Regression Results                  \n","==============================================================================\n","Dep. Variable:                  count   No. Observations:                  100\n","Model:                            GLM   Df Residuals:                       97\n","Model Family:                 Poisson   Df Model:                            2\n","Link Function:                    Log   Scale:                          1.0000\n","Method:                          IRLS   Log-Likelihood:                -135.63\n","Date:                Tue, 11 Jun 2024   Deviance:                       118.72\n","Time:                        20:15:45   Pearson chi2:                     102.\n","No. Iterations:                     5   Pseudo R-squ. (CS):             0.1093\n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          z      P>|z|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const         -0.1509      0.252     -0.600      0.549      -0.644       0.342\n","feature1       0.9652      0.319      3.024      0.002       0.340       1.591\n","feature2      -0.5017      0.340     -1.476      0.140      -1.168       0.164\n","==============================================================================\n","\n","各特徴量の係数:\n","const      -0.150915\n","feature1    0.965217\n","feature2   -0.501731\n","dtype: float64\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import statsmodels.api as sm\n","from sklearn.datasets import make_regression\n","\n","# データセットの作成（カウントデータを模倣）\n","np.random.seed(42)\n","n_samples = 100\n","X = np.random.rand(n_samples, 2)  # 2つの特徴量\n","beta = np.array([0.5, -0.3])\n","lin_pred = np.dot(X, beta)\n","y = np.random.poisson(lam=np.exp(lin_pred))\n","\n","# データフレームの作成\n","data = pd.DataFrame(X, columns=['feature1', 'feature2'])\n","data['count'] = y\n","\n","# ポアソン回帰モデルの作成とフィッティング\n","X = sm.add_constant(data[['feature1', 'feature2']])  # 切片項を追加\n","poisson_model = sm.GLM(data['count'], X, family=sm.families.Poisson()).fit()\n","\n","# モデルの要約を表示\n","print(poisson_model.summary())\n","\n","# 各特徴量の影響（係数）の表示\n","coefficients = poisson_model.params\n","print(\"\\n各特徴量の係数:\")\n","print(coefficients)"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
