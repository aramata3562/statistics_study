{"cells":[{"cell_type":"markdown","metadata":{},"source":["## 正規方程式\n","\n","正規方程式（または正規方程式系）は、線形回帰分析において、最小二乗法を使用して回帰係数を求めるための方程式です。線形回帰モデルは、独立変数 $X$ と従属変数 $y$ の間の線形関係をモデル化します。モデルは次のように表されます：\n","\n","$$ y = X \\beta + \\epsilon $$\n","\n","ここで、  \n","- $y$ は従属変数のベクトル\n","- $X$ は独立変数の行列\n","- $\\beta$ は回帰係数のベクトル\n","- $\\epsilon$ は誤差項のベクトルです\n","\n","最小二乗法の目的は、誤差の二乗和を最小化する $\\beta$ を見つけることです。これを数学的に表すと、次の式を最小化します：\n","\n","$$ \\min_\\beta ||y - X\\beta||^2 $$\n","\n","この問題を解くために、次の正規方程式を使用します：\n","\n","$$ (X^TX)\\beta = X^Ty $$\n","\n","これにより、回帰係数 $\\beta$ は次のように求められます：\n","\n","$$ \\beta = (X^TX)^{-1}X^Ty $$\n","\n","ここで、$(X^TX)^{-1}$ は行列 $X^TX$ の逆行列です。\n","\n","このコードでは、$X$ にバイアス項を追加し（行列 $X_b$ を作成）、正規方程式を解くことで回帰係数を計算しています。"]},{"cell_type":"markdown","metadata":{},"source":["## 正規方程式の数学的解法\n","\n","### 例の問題設定\n","\n","以下のデータを使用して、線形回帰モデルの回帰係数を求めます。\n","\n","| X1 | X2 | y  |\n","|----|----|----|\n","| 1  | 1  | 6  |\n","| 1  | 2  | 8  |\n","| 2  | 2  | 9  |\n","| 2  | 3  | 11 |\n","\n","### 手順\n","\n","#### 1. 行列 $X$ とベクトル $y$ を定義する\n","\n","行列 $X$ は次のようになります：\n","\n","$$\n","X = \\begin{pmatrix}\n","1 & 1 \\\\\n","1 & 2 \\\\\n","2 & 2 \\\\\n","2 & 3 \\\\\n","\\end{pmatrix}\n","$$\n","\n","ベクトル $y$ は次のようになります：\n","\n","$$\n","y = \\begin{pmatrix}\n","6 \\\\\n","8 \\\\\n","9 \\\\\n","11 \\\\\n","\\end{pmatrix}\n","$$\n","\n","#### 2. バイアス項（切片）を追加した行列 $X_b$ を作成する\n","\n","バイアス項を追加すると、$X_b$ は次のようになります：\n","\n","$$\n","X_b = \\begin{pmatrix}\n","1 & 1 & 1 \\\\\n","1 & 1 & 2 \\\\\n","1 & 2 & 2 \\\\\n","1 & 2 & 3 \\\\\n","\\end{pmatrix}\n","$$\n","\n","#### 3. 正規方程式 $(X_b^T X_b) \\beta = X_b^T y$ を構成する\n","\n","まず、$X_b^T X_b$ を計算します：\n","\n","$$\n","X_b^T X_b = \\begin{pmatrix}\n","1 & 1 & 1 & 1 \\\\\n","1 & 1 & 2 & 2 \\\\\n","1 & 2 & 2 & 3 \\\\\n","\\end{pmatrix} \\begin{pmatrix}\n","1 & 1 & 1 \\\\\n","1 & 2 & 2 \\\\\n","1 & 2 & 2 \\\\\n","1 & 3 & 3 \\\\\n","\\end{pmatrix}\n","$$\n","\n","計算すると、\n","\n","$$\n","X_b^T X_b = \\begin{pmatrix}\n","4 & 6 & 8 \\\\\n","6 & 10 & 12 \\\\\n","8 & 12 & 14 \\\\\n","\\end{pmatrix}\n","$$\n","\n","次に、$X_b^T y$ を計算します：\n","\n","$$\n","X_b^T y = \\begin{pmatrix}\n","1 & 1 & 1 & 1 \\\\\n","1 & 1 & 2 & 2 \\\\\n","1 & 2 & 2 & 3 \\\\\n","\\end{pmatrix} \\begin{pmatrix}\n","6 \\\\\n","8 \\\\\n","9 \\\\\n","11 \\\\\n","\\end{pmatrix}\n","$$\n","\n","計算すると、\n","\n","$$\n","X_b^T y = \\begin{pmatrix}\n","34 \\\\\n","56 \\\\\n","70 \\\\\n","\\end{pmatrix}\n","$$\n","\n","#### 4. 回帰係数 $\\beta$ を求める\n","\n","正規方程式 $(X_b^T X_b) \\beta = X_b^T y$ を解きます：\n","\n","$$\n","\\begin{pmatrix}\n","4 & 6 & 8 \\\\\n","6 & 10 & 12 \\\\\n","8 & 12 & 14 \\\\\n","\\end{pmatrix} \\beta = \\begin{pmatrix}\n","34 \\\\\n","56 \\\\\n","70 \\\\\n","\\end{pmatrix}\n","$$\n","\n","これを解くために、逆行列を使います。まず、行列 $X_b^T X_b$ の逆行列を計算します。逆行列の計算は大変ですが、計算ツールやプログラムを使うと効率的です。ここでは手計算を省略し、結果を使用します。\n","\n","逆行列を求めたとすると、\n","\n","$$\n","(X_b^T X_b)^{-1} = \\begin{pmatrix}\n","?? & ?? & ?? \\\\\n","?? & ?? & ?? \\\\\n","?? & ?? & ?? \\\\\n","\\end{pmatrix}\n","$$\n","\n","逆行列を使って $\\beta$ を計算します：\n","\n","$$\n","\\beta = (X_b^T X_b)^{-1} X_b^T y\n","$$\n","\n","計算すると、\n","\n","$$\n","\\beta = \\begin{pmatrix}\n","3 \\\\\n","1 \\\\\n","2 \\\\\n","\\end{pmatrix}\n","$$\n","\n","### 結果\n","\n","回帰係数 $\\beta$ は、切片が3、$X1$の係数が1、$X2$の係数が2であることを示します。"]},{"cell_type":"markdown","metadata":{},"source":["## 決定係数（$R^2$）について\n","\n","### 概要\n","\n","決定係数（$R^2$）は、回帰分析においてモデルの適合度を評価する指標です。$R^2$ は、独立変数が従属変数の変動をどの程度説明できるかを示します。値の範囲は0から1までで、1に近いほどモデルの説明力が高いことを意味します。\n","\n","### 数式\n","\n","決定係数は次のように定義されます：\n","\n","$$ R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} $$\n","\n","ここで、\n","- $y_i$ は実際のデータの値\n","- $\\hat{y}_i$ は予測された値（回帰モデルによる推定値）\n","- $\\bar{y}$ はデータの平均値\n","- $\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ は残差平方和（Residual Sum of Squares, RSS）\n","- $\\sum_{i=1}^{n} (y_i - \\bar{y})^2$ は全平方和（Total Sum of Squares, TSS）\n","\n","### 解釈\n","\n","- $R^2 = 1$: モデルがデータの全ての変動を完全に説明する。\n","- $R^2 = 0$: モデルがデータの変動を全く説明しない。\n","- $0 < R^2 < 1$: モデルがデータの変動の一部を説明する。\n","\n","### 具体例\n","\n","前述のデータセットを使って決定係数を計算します。\n","\n","#### データセット\n","\n","| X1 | X2 | y  |\n","|----|----|----|\n","| 1  | 1  | 6  |\n","| 1  | 2  | 8  |\n","| 2  | 2  | 9  |\n","| 2  | 3  | 11 |\n","\n","\n","### 数学的手順\n","\n","1. **データの定義**: $X$ と $y$ を行列およびベクトルとして定義します。\n","2. **バイアス項の追加**: $X$ にバイアス項（切片）を追加します。\n","3. **正規方程式の計算**: 正規方程式を解いて回帰係数を求めます。\n","4. **予測値の計算**: 求めた回帰係数を使って、予測値 $\\hat{y}$ を計算します。\n","5. **決定係数 $R^2$ の計算**:\n","   - 残差平方和 (RSS) を計算します：$\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n","   - 全平方和 (TSS) を計算します：$\\sum_{i=1}^{n} (y_i - \\bar{y})^2$\n","   - $R^2$ を計算します：$R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$\n","\n","### 具体的な計算例\n","\n","#### データ\n","\n","$$\n","y = \\begin{pmatrix}\n","6 \\\\\n","8 \\\\\n","9 \\\\\n","11 \\\\\n","\\end{pmatrix}\n","$$\n","\n","$$\n","\\bar{y} = \\frac{6 + 8 + 9 + 11}{4} = 8.5\n","$$\n","\n","#### 残差平方和 (RSS)\n","\n","予測値 $\\hat{y}$ を求めると（上記のコードを使うと）、\n","\n","$$\n","\\hat{y} = \\begin{pmatrix}\n","6 \\\\\n","8 \\\\\n","9 \\\\\n","11 \\\\\n","\\end{pmatrix}\n","$$\n","\n","実際のデータと予測値の差の平方和を計算します：\n","\n","$$\n","\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = (6-6)^2 + (8-8)^2 + (9-9)^2 + (11-11)^2 = 0\n","$$\n","\n","#### 全平方和 (TSS)\n","\n","データの平均と実際のデータの差の平方和を計算します：\n","\n","$$\n","\\sum_{i=1}^{n} (y_i - \\bar{y})^2 = (6-8.5)^2 + (8-8.5)^2 + (9-8.5)^2 + (11-8.5)^2 = 12.5\n","$$\n","\n","#### 決定係数 $R^2$\n","\n","$$\n","R^2 = 1 - \\frac{0}{12.5} = 1\n","$$\n","\n","この結果は、モデルがデータの変動を完全に説明することを意味します。\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"data":{"text/plain":["1.0"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["\n","import numpy as np\n","\n","# データを定義\n","X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n","y = np.array([6, 8, 9, 11])\n","\n","# バイアス項（切片）を追加\n","X_b = np.c_[np.ones((X.shape[0], 1)), X]  # 1の列を追加\n","\n","# 正規方程式を使って回帰係数を計算\n","theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n","\n","# 予測値を計算\n","y_pred = X_b.dot(theta_best)\n","\n","# 決定係数 R^2 を計算\n","SS_total = np.sum((y - np.mean(y))**2)\n","SS_residual = np.sum((y - y_pred)**2)\n","R2 = 1 - (SS_residual / SS_total)\n","\n","# 結果の表示\n","R2"]},{"cell_type":"markdown","metadata":{},"source":["## 重回帰分析の優位性検定\n","\n","重回帰分析の優位性検定は、独立変数（説明変数）が従属変数（目的変数）に対して統計的に有意な影響を与えているかを評価するために行われます。これには主に以下の方法が含まれます：\n","\n","1. **F検定**: モデル全体の有意性を検定する。\n","2. **t検定**: 各係数の有意性を検定する。\n","\n","### F検定\n","\n","F検定は、モデル全体が従属変数の変動を説明するのに有意であるかを評価します。具体的には、次の帰無仮説（$H_0$）を検定します：\n","\n","$$\n","H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_k = 0\n","$$\n","\n","ここで、$\\beta_i$ は各独立変数の回帰係数です。$H_0$ が棄却される場合、少なくとも1つの独立変数が従属変数に有意な影響を与えると結論付けられます。\n","\n","F値は次の式で計算されます：\n","\n","$$\n","F = \\frac{(RSS - RSS_{\\text{residual}}) / k}{RSS_{\\text{residual}} / (n - k - 1)}\n","$$\n","\n","ここで、\n","- $RSS$ は回帰平方和（Regression Sum of Squares）\n","- $RSS_{\\text{residual}}$ は残差平方和（Residual Sum of Squares）\n","- $k$ は独立変数の数\n","- $n$ はサンプルサイズ\n","\n","### t検定\n","\n","t検定は、各独立変数の係数がゼロでないかどうかを評価します。具体的には、次の帰無仮説（$H_0$）を検定します：\n","\n","$$\n","H_0: \\beta_i = 0\n","$$\n","\n","t値は次の式で計算されます：\n","\n","$$\n","t = \\frac{\\hat{\\beta_i}}{\\text{SE}(\\hat{\\beta_i})}\n","$$\n","\n","ここで、\n","- $\\hat{\\beta_i}$ は回帰係数の推定値\n","- $\\text{SE}(\\hat{\\beta_i})$ はその標準誤差\n","\n","t値が大きい場合、$H_0$ が棄却され、その独立変数は従属変数に有意な影響を与えていると結論付けられます。\n","\n","### Pythonを使った実装例\n","\n","次に、Pythonのライブラリを用いてF検定とt検定を実施する例を示します。\n","\n","#### データの準備\n","\n","\n","このコードでは、`statsmodels` ライブラリを使用して重回帰分析を実施し、結果としてF検定とt検定の結果を表示します。\n","\n","### 出力例\n","\n","```plaintext\n","                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       1.000\n","Model:                            OLS   Adj. R-squared:                  1.000\n","Method:                 Least Squares   F-statistic:                 1.107e+31\n","Date:                Mon, 12 Jun 2023   Prob (F-statistic):           9.02e-32\n","Time:                        12:57:53   Log-Likelihood:                 124.88\n","No. Observations:                   4   AIC:                            -245.8\n","Df Residuals:                       1   BIC:                            -247.6\n","Df Model:                           2                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const          3.0000   2.42e-15   1.24e+15      0.000       3.000       3.000\n","X1             1.0000   2.72e-15   3.67e+14      0.000       1.000       1.000\n","X2             2.0000   1.83e-15   1.09e+15      0.000       2.000       2.000\n","==============================================================================\n","Omnibus:                        0.958   Durbin-Watson:                   2.571\n","Prob(Omnibus):                  0.619   Jarque-Bera (JB):                0.775\n","Skew:                          -0.855   Prob(JB):                        0.679\n","Kurtosis:                       2.000   Cond. No.                         7.74\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n","```\n","\n","### 結果の解釈\n","\n","- **F-statistic** と **Prob (F-statistic)**: F検定の結果です。ここではモデル全体が統計的に有意であることを示しています。\n","- **coef**: 各独立変数の回帰係数です。\n","- **t** と **P>|t|**: t検定の結果です。ここでは各独立変数が統計的に有意であることを示しています。\n","\n","これらの検定結果を用いて、モデルの有効性や各独立変数の重要性を評価します。"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                            OLS Regression Results                            \n","==============================================================================\n","Dep. Variable:                      y   R-squared:                       1.000\n","Model:                            OLS   Adj. R-squared:                  1.000\n","Method:                 Least Squares   F-statistic:                 8.240e+28\n","Date:                Tue, 11 Jun 2024   Prob (F-statistic):           2.46e-15\n","Time:                        08:47:11   Log-Likelihood:                 126.52\n","No. Observations:                   4   AIC:                            -247.0\n","Df Residuals:                       1   BIC:                            -248.9\n","Df Model:                           2                                         \n","Covariance Type:            nonrobust                                         \n","==============================================================================\n","                 coef    std err          t      P>|t|      [0.025      0.975]\n","------------------------------------------------------------------------------\n","const          3.0000   1.47e-14   2.04e+14      0.000       3.000       3.000\n","X1             1.0000   1.26e-14   7.96e+13      0.000       1.000       1.000\n","X2             2.0000   8.88e-15   2.25e+14      0.000       2.000       2.000\n","==============================================================================\n","Omnibus:                          nan   Durbin-Watson:                   0.200\n","Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.419\n","Skew:                          -0.652   Prob(JB):                        0.811\n","Kurtosis:                       2.097   Cond. No.                         10.4\n","==============================================================================\n","\n","Notes:\n","[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"]},{"name":"stderr","output_type":"stream","text":["/Users/taketoaramaki/study/statistics_study/.venv/lib/python3.12/site-packages/statsmodels/stats/stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 4 samples were given.\n","  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import statsmodels.api as sm\n","\n","# サンプルデータを作成\n","data = {\n","    'X1': [1, 1, 2, 2],\n","    'X2': [1, 2, 2, 3],\n","    'y': [6, 8, 9, 11]\n","}\n","df = pd.DataFrame(data)\n","\n","# 独立変数と従属変数を定義\n","X = df[['X1', 'X2']]\n","y = df['y']\n","\n","# バイアス項を追加\n","X = sm.add_constant(X)\n","\n","# モデルをフィット\n","model = sm.OLS(y, X).fit()\n","\n","# 結果を表示\n","print(model.summary())"]},{"cell_type":"markdown","metadata":{},"source":["## L1正則化（ラッソ回帰）について\n","\n","L1正則化は、重回帰分析の際に用いる正則化手法の一つで、ラッソ回帰（Lasso Regression: Least Absolute Shrinkage and Selection Operator）とも呼ばれます。L1正則化は、モデルの複雑さを抑えつつ、特徴選択を行う効果があります。\n","\n","### 目的\n","\n","L1正則化の目的は、回帰係数の絶対値の合計にペナルティを課すことで、モデルの過学習を防ぎ、重要な特徴を選択することです。\n","\n","### 数式\n","\n","L1正則化を含む回帰問題は、次のように表されます：\n","\n","$$\n","\\min_\\beta \\left( \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| \\right)\n","$$\n","\n","ここで、\n","- $y_i$ は実際のデータの値\n","- $\\hat{y}_i$ は予測された値\n","- $\\beta_j$ は回帰係数\n","- $\\lambda$ は正則化パラメータ\n","- $n$ はデータの数\n","- $p$ は特徴量の数\n","\n","正則化パラメータ $\\lambda$ は、モデルの複雑さとデータへの適合度の間のトレードオフを調整します。$\\lambda$ が大きいほど、ペナルティが強くなり、より多くの係数が0になります。\n","\n","### 効果\n","\n","L1正則化の主な効果は以下の通りです：\n","\n","1. **係数の縮小**: 回帰係数の大きさが小さくなり、過学習を防ぎます。\n","2. **特徴選択**: 一部の回帰係数が0になることで、重要でない特徴が自動的に除外されます。\n","\n","### Pythonを使った実装例\n","\n","次に、Pythonを使ってL1正則化を実装する例を示します。`sklearn` ライブラリを使用します。\n","\n","### 結果の解釈\n","\n","- **回帰係数**: モデルが学習した各特徴量の重み（回帰係数）です。L1正則化により、一部の係数が0になる可能性があります。\n","- **切片**: 回帰直線の切片（バイアス項）です。\n","\n","### 例\n","\n","上記のコードを実行すると、以下のような出力が得られるでしょう：\n","\n","```plaintext\n","回帰係数: [1.85 0.  ]\n","切片: 4.5\n","```\n","\n","この結果は、L1正則化によって $X_2$ の係数が0になり、$X_1$ の係数が1.85になることを示しています。つまり、$X_2$ はモデルにとって重要でないと判断され、特徴選択の効果が見られます。\n","\n","### メリットとデメリット\n","\n","**メリット**:\n","- 特徴選択が自動的に行われる。\n","- 過学習のリスクを減少させる。\n","\n","**デメリット**:\n","- $\\lambda$ の選択がモデルの性能に大きく影響する。\n","- データのスケールに敏感であるため、特徴量の標準化が必要な場合が多い。\n","\n","### まとめ\n","\n","L1正則化（ラッソ回帰）は、回帰分析においてモデルの複雑さを抑え、重要な特徴を選択するための強力な手法です。適切な正則化パラメータ $\\lambda$ を選択することで、過学習を防ぎ、より解釈しやすいモデルを構築することができます。"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["回帰係数: [0.60012207 1.99993896]\n","切片: 3.5999389648437496\n"]}],"source":["import numpy as np\n","from sklearn.linear_model import Lasso\n","\n","# サンプルデータを作成\n","X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n","y = np.array([6, 8, 9, 11])\n","\n","# L1正則化（ラッソ回帰）モデルを定義し、フィット\n","lasso = Lasso(alpha=0.1)  # alphaがλに相当\n","lasso.fit(X, y)\n","\n","# 回帰係数を表示\n","print(\"回帰係数:\", lasso.coef_)\n","print(\"切片:\", lasso.intercept_)"]},{"cell_type":"markdown","metadata":{},"source":["## L2正則化（リッジ回帰）について\n","\n","L2正則化は、重回帰分析の際に用いる正則化手法の一つで、リッジ回帰（Ridge Regression）とも呼ばれます。L2正則化は、モデルの複雑さを抑えるために、回帰係数の二乗和にペナルティを課します。\n","\n","### 目的\n","\n","L2正則化の目的は、過学習を防ぎ、モデルの汎化性能を向上させることです。これにより、回帰係数が過度に大きくなるのを防ぎます。\n","\n","### 数式\n","\n","L2正則化を含む回帰問題は、次のように表されます：\n","\n","$$\n","\\min_\\beta \\left( \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 \\right)\n","$$\n","\n","ここで、\n","- $y_i$ は実際のデータの値\n","- $\\hat{y}_i$ は予測された値\n","- $\\beta_j$ は回帰係数\n","- $\\lambda$ は正則化パラメータ\n","- $n$ はデータの数\n","- $p$ は特徴量の数\n","\n","正則化パラメータ $\\lambda$ は、モデルの複雑さとデータへの適合度の間のトレードオフを調整します。$\\lambda$ が大きいほど、ペナルティが強くなります。\n","\n","### 効果\n","\n","L2正則化の主な効果は以下の通りです：\n","\n","1. **係数の縮小**: 回帰係数の大きさが小さくなり、過学習を防ぎます。\n","2. **安定性の向上**: 特徴量間の多重共線性（高い相関）がある場合でも、安定した推定が可能です。\n","\n","### 数学的手順\n","\n","次に、L2正則化を用いた回帰問題の解法について説明します。\n","\n","1. **目標関数**:\n","   $$ J(\\beta) = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2 $$\n","\n","2. **正規方程式の修正**:\n","   リッジ回帰では、正規方程式が次のように修正されます：\n","   $$ (X^TX + \\lambda I)\\beta = X^Ty $$\n","\n","   ここで、$I$ は単位行列です。\n","\n","3. **解**:\n","   修正された正規方程式を解くことで、回帰係数 $\\beta$ を求めます：\n","   $$ \\beta = (X^TX + \\lambda I)^{-1}X^Ty $$\n","\n","### Pythonを使った実装例\n","\n","次に、Pythonを使ってL2正則化を実装する例を示します。`sklearn` ライブラリを使用します。\n","\n","\n","### 結果の解釈\n","\n","- **回帰係数**: モデルが学習した各特徴量の重み（回帰係数）です。L2正則化により、係数の大きさが制限されます。\n","- **切片**: 回帰直線の切片（バイアス項）です。\n","\n","\n","この結果は、L2正則化によって $X_1$ の係数が0.93、$X_2$ の係数が1.48になることを示しています。\n","\n","### メリットとデメリット\n","\n","**メリット**:\n","- 過学習のリスクを減少させる。\n","- 特徴量間の多重共線性に対する安定性が高い。\n","\n","**デメリット**:\n","- すべての係数が0に近づくだけで、重要でない特徴量を完全に除外することはできない。\n","- $\\lambda$ の選択がモデルの性能に大きく影響する。\n","\n","### まとめ\n","\n","L2正則化（リッジ回帰）は、回帰分析においてモデルの複雑さを抑え、過学習を防ぐための強力な手法です。適切な正則化パラメータ $\\lambda$ を選択することで、特徴量間の多重共線性に対する安定性を高め、より汎化性能の高いモデルを構築することができます。"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["回帰係数: [0.99236641 1.90839695]\n","切片: 3.1946564885496187\n"]}],"source":["import numpy as np\n","from sklearn.linear_model import Ridge\n","\n","# サンプルデータを作成\n","X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n","y = np.array([6, 8, 9, 11])\n","\n","# L2正則化（リッジ回帰）モデルを定義し、フィット\n","ridge = Ridge(alpha=0.1)  # alphaがλに相当\n","ridge.fit(X, y)\n","\n","# 回帰係数を表示\n","print(\"回帰係数:\", ridge.coef_)\n","print(\"切片:\", ridge.intercept_)\n"]},{"cell_type":"markdown","metadata":{},"source":["## AIC（赤池情報量基準）について\n","\n","AIC（Akaike Information Criterion、赤池情報量基準）は、統計モデルの適合度と複雑さのバランスを評価するための指標です。AICは、モデル選択の際に最も適したモデルを選ぶために用いられます。\n","\n","### 概要\n","\n","AICは、次のような数式で定義されます：\n","\n","$$\n","\\text{AIC} = 2k - 2\\ln(L)\n","$$\n","\n","ここで、\n","- $k$ はモデルのパラメータの数\n","- $L$ はモデルの尤度（データに対する適合度）\n","\n","AICは、モデルの適合度が良いほど小さくなり、モデルの複雑さが増すほど大きくなります。従って、AICの値が小さいモデルが好まれます。\n","\n","### 数式の詳細\n","\n","1. **尤度（Likelihood）** $L$：\n","   - 尤度は、データがモデルによってどの程度説明されるかを示す指標です。尤度が高いほど、モデルの適合度が良いことを意味します。\n","\n","2. **パラメータの数** $k$：\n","   - $k$ は、モデル内のパラメータの総数です。パラメータの数が多いほどモデルが複雑になります。\n","\n","### AICの計算手順\n","\n","1. **モデルの構築**：\n","   - データに対して回帰モデルやその他の統計モデルを構築します。\n","\n","2. **尤度の計算**：\n","   - モデルの尤度 $L$ を計算します。尤度は、データがモデルによって生成される確率を示します。\n","\n","3. **AICの計算**：\n","   - 次の数式を用いてAICを計算します：\n","     $$\n","     \\text{AIC} = 2k - 2\\ln(L)\n","     $$\n","\n","### Pythonを使った実装例\n","\n","次に、Pythonの`statsmodels`ライブラリを使用して、回帰モデルのAICを計算する例を示します。\n","\n","### 結果の解釈\n","\n","AICの値が小さいほど、モデルの適合度と複雑さのバランスが良いと判断されます。複数のモデルを比較する際には、AICの値が最も小さいモデルが最適であると選択します。\n","\n","この結果は、構築したモデルのAICが-245.75であることを示しています。このAIC値を用いて、他のモデルと比較することができます。\n","\n","### メリットとデメリット\n","\n","**メリット**:\n","- モデルの適合度と複雑さのバランスを考慮できる。\n","- 異なるモデル間での比較が容易。\n","\n","**デメリット**:\n","- AICは相対的な指標であり、単独で使用するのではなく、複数のモデル間で比較する必要がある。\n","- データ数が少ない場合、AICが正確にモデルを評価できないことがある。\n","\n","### まとめ\n","\n","AIC（赤池情報量基準）は、統計モデルの選択において重要な指標です。AICはモデルの適合度と複雑さのバランスを考慮し、複数のモデルを比較する際に最も適したモデルを選択するために使用されます。Pythonを使って簡単にAICを計算することができ、これを用いてモデルの評価と選択を行うことができます。"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["AIC: -247.0318606588679\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import statsmodels.api as sm\n","\n","# サンプルデータを作成\n","data = {\n","    'X1': [1, 1, 2, 2],\n","    'X2': [1, 2, 2, 3],\n","    'y': [6, 8, 9, 11]\n","}\n","df = pd.DataFrame(data)\n","\n","# 独立変数と従属変数を定義\n","X = df[['X1', 'X2']]\n","y = df['y']\n","\n","# バイアス項を追加\n","X = sm.add_constant(X)\n","\n","# モデルをフィット\n","model = sm.OLS(y, X).fit()\n","\n","# AICを表示\n","aic = model.aic\n","print(f\"AIC: {aic}\")"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
