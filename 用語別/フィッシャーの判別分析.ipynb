{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## フィッシャーの判別分析について\n",
    "\n",
    "フィッシャーの判別分析（Fisher's Linear Discriminant Analysis, LDA）は、統計学と機械学習において、2つ以上のグループにデータを分類するための線形手法です。主に、特徴量空間をグループ間の分離を最大化する方向に変換し、新しい観測データのグループを予測するために使用されます。\n",
    "\n",
    "### 基本概念\n",
    "\n",
    "1. **目標**: 異なるクラス間の分離を最大化し、同一クラス内のデータのばらつきを最小化する線形判別関数を求める。\n",
    "2. **線形判別関数**: データポイント $\\mathbf{x}$ を分類するための線形関数 $y = \\mathbf{w}^T \\mathbf{x} + b$ を求めます。ここで、$\\mathbf{w}$ は重みベクトル、$b$ はバイアス項です。\n",
    "\n",
    "### ステップ\n",
    "\n",
    "1. **データの準備**: まず、各クラスに属するサンプルデータを集めます。\n",
    "2. **平均ベクトルの計算**: 各クラス $i$ について、平均ベクトル $\\mathbf{m}_i$ を計算します。\n",
    "   $$\n",
    "   \\mathbf{m}_i = \\frac{1}{N_i} \\sum_{k=1}^{N_i} \\mathbf{x}_k\n",
    "   $$\n",
    "   ここで、$N_i$ はクラス $i$ のサンプル数、$\\mathbf{x}_k$ はクラス $i$ に属するサンプルデータです。\n",
    "\n",
    "3. **クラス内分散行列の計算**: 各クラス内のばらつきを表す分散行列 $S_i$ を計算します。\n",
    "   $$\n",
    "   S_i = \\sum_{k=1}^{N_i} (\\mathbf{x}_k - \\mathbf{m}_i)(\\mathbf{x}_k - \\mathbf{m}_i)^T\n",
    "   $$\n",
    "\n",
    "4. **総クラス内分散行列の計算**: 全クラスのクラス内分散行列を合計します。\n",
    "   $$\n",
    "   S_W = \\sum_{i} S_i\n",
    "   $$\n",
    "\n",
    "5. **クラス間分散行列の計算**: クラスの平均ベクトル間のばらつきを表すクラス間分散行列 $S_B$ を計算します。\n",
    "   $$\n",
    "   S_B = \\sum_{i} N_i (\\mathbf{m}_i - \\mathbf{m})(\\mathbf{m}_i - \\mathbf{m})^T\n",
    "   $$\n",
    "   ここで、$\\mathbf{m}$ は全データの平均ベクトルです。\n",
    "\n",
    "6. **最適な変換ベクトル $\\mathbf{w}$ の計算**: \n",
    "   $$\n",
    "   \\mathbf{w} = S_W^{-1} (\\mathbf{m}_1 - \\mathbf{m}_2)\n",
    "   $$\n",
    "   このベクトル $\\mathbf{w}$ によって、データを1次元に射影し、クラス間の分離を最大化します。\n",
    "\n",
    "### 実装例\n",
    "\n",
    "以下は、Pythonでフィッシャーの判別分析を実装する例です。`scikit-learn`ライブラリを使用します。\n",
    "\n",
    "\n",
    "このコードでは、Irisデータセットを使用してLDAを適用し、2次元空間に射影した結果をプロットしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# サンプルデータの作成\n",
    "np.random.seed(0)\n",
    "class_1 = np.random.randn(20, 2) + np.array([0, 0])\n",
    "class_2 = np.random.randn(20, 2) + np.array([5, 5])\n",
    "\n",
    "X = np.vstack((class_1, class_2))\n",
    "y = np.hstack((np.zeros(20), np.ones(20)))\n",
    "\n",
    "# LDAモデルの作成と適用\n",
    "lda = LDA()\n",
    "lda.fit(X, y)\n",
    "X_lda = lda.transform(X)\n",
    "\n",
    "# プロット\n",
    "plt.scatter(X_lda[y == 0], np.zeros(20), label='Class 1')\n",
    "plt.scatter(X_lda[y == 1], np.zeros(20), label='Class 2')\n",
    "plt.xlabel('LD1')\n",
    "plt.ylabel('LD2')\n",
    "plt.legend()\n",
    "plt.title('Fisher\\'s Linear Discriminant Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## フィッシャーの判別分析の具体的な計算過程\n",
    "\n",
    "具体的な例を用いて、フィッシャーの判別分析の計算過程を示します。ここでは、簡単な2クラスのデータセットを使用します。\n",
    "\n",
    "### データセット\n",
    "以下のような2クラスのデータを考えます：\n",
    "- クラス0: $(2, 3), (3, 3), (2, 2)$\n",
    "- クラス1: $(6, 8), (7, 9), (8, 8)$\n",
    "\n",
    "### ステップ1: クラスごとの平均ベクトルの計算\n",
    "\n",
    "クラス0の平均ベクトル $\\mu_0$ とクラス1の平均ベクトル $\\mu_1$ を計算します。\n",
    "$$\n",
    "\\mu_0 = \\frac{1}{3} \\left( \\begin{array}{c}\n",
    "2 + 3 + 2 \\\\\n",
    "3 + 3 + 2 \\\\\n",
    "\\end{array} \\right)\n",
    "= \\left( \\begin{array}{c}\n",
    "2.33 \\\\\n",
    "2.67 \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_1 = \\frac{1}{3} \\left( \\begin{array}{c}\n",
    "6 + 7 + 8 \\\\\n",
    "8 + 9 + 8 \\\\\n",
    "\\end{array} \\right)\n",
    "= \\left( \\begin{array}{c}\n",
    "7.00 \\\\\n",
    "8.33 \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "### ステップ2: クラス内散布行列 $S_W$ の計算\n",
    "\n",
    "各クラスの散布行列を計算し、それらを合計します。\n",
    "\n",
    "クラス0の散布行列 $S_0$：\n",
    "$$\n",
    "S_0 = \\sum_{i=1}^{3} (x_i - \\mu_0)(x_i - \\mu_0)^T\n",
    "= \\left( \\begin{array}{cc}\n",
    "0.22 & 0.22 \\\\\n",
    "0.22 & 0.67 \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "クラス1の散布行列 $S_1$：\n",
    "$$\n",
    "S_1 = \\sum_{i=1}^{3} (x_i - \\mu_1)(x_i - \\mu_1)^T\n",
    "= \\left( \\begin{array}{cc}\n",
    "2.00 & 1.00 \\\\\n",
    "1.00 & 0.67 \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "クラス内散布行列 $S_W$：\n",
    "$$\n",
    "S_W = S_0 + S_1 = \\left( \\begin{array}{cc}\n",
    "2.22 & 1.22 \\\\\n",
    "1.22 & 1.34 \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "### ステップ3: クラス間散布行列 $S_B$ の計算\n",
    "\n",
    "クラス間散布行列 $S_B$ を計算します。\n",
    "$$\n",
    "S_B = N_0 (\\mu_0 - \\mu)(\\mu_0 - \\mu)^T + N_1 (\\mu_1 - \\mu)(\\mu_1 - \\mu)^T\n",
    "$$\n",
    "\n",
    "ここで、全体の平均ベクトル $\\mu$ は次のように計算されます：\n",
    "$$\n",
    "\\mu = \\frac{N_0 \\mu_0 + N_1 \\mu_1}{N_0 + N_1} = \\left( \\begin{array}{c}\n",
    "4.67 \\\\\n",
    "5.50 \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "したがって、\n",
    "$$\n",
    "S_B = \\left( \\begin{array}{cc}\n",
    "18.67 & 21.00 \\\\\n",
    "21.00 & 23.67 \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "### ステップ4: 判別関数の計算\n",
    "\n",
    "固有値問題 $S_W^{-1} S_B w = \\lambda w$ を解きます。\n",
    "\n",
    "逆行列 $S_W^{-1}$ を計算します：\n",
    "$$\n",
    "S_W^{-1} = \\left( \\begin{array}{cc}\n",
    "6.0 & -5.5 \\\\\n",
    "-5.5 & 10.0 \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "次に、$S_W^{-1} S_B$ を計算します：\n",
    "$$\n",
    "S_W^{-1} S_B = \\left( \\begin{array}{cc}\n",
    "48.0 & 54.0 \\\\\n",
    "-24.5 & -27.0 \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "この行列の固有値と固有ベクトルを計算すると、最大の固有値に対応する固有ベクトル $w$ は次のようになります：\n",
    "$$\n",
    "w = \\left( \\begin{array}{c}\n",
    "0.70 \\\\\n",
    "0.71 \\\\\n",
    "\\end{array} \\right)\n",
    "$$\n",
    "\n",
    "このベクトル $w$ が判別関数の係数となり、データを次元削減するために使用されます。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
