{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## バイアス・バリアンス分解について\n",
    "\n",
    "バイアス・バリアンス分解（Bias-Variance Decomposition）は、統計モデルの誤差（期待損失）を理解し、モデルの性能を評価するための概念です。モデルの誤差をバイアス（偏り）とバリアンス（分散）の2つの要素に分解することで、モデルの予測能力や過学習・過少学習の特性を分析することができます。\n",
    "\n",
    "### モデルの誤差の分解\n",
    "\n",
    "モデルの予測誤差（平均二乗誤差）は、次の3つの要素に分解されます：\n",
    "\n",
    "1. **バイアス（偏り）**：モデルの予測値と真の値の差の期待値。\n",
    "2. **バリアンス（分散）**：モデルの予測値のばらつき。\n",
    "3. **ノイズ（誤差の分散）**：データの固有のランダムな誤差。\n",
    "\n",
    "#### 数式による表現\n",
    "\n",
    "まず、$Y$ を予測対象の真の値、$X$ を説明変数、$\\hat{f}(X)$ をモデルの予測値とします。予測誤差（期待損失）は次のように表されます：\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[(Y - \\hat{f}(X))^2\\right]\n",
    "$$\n",
    "\n",
    "この予測誤差を以下のように分解できます：\n",
    "\n",
    "$$\n",
    "\\mathbb{E}\\left[(Y - \\hat{f}(X))^2\\right] = (\\text{Bias}[\\hat{f}(X)])^2 + \\text{Var}[\\hat{f}(X)] + \\sigma^2\n",
    "$$\n",
    "\n",
    "ここで、\n",
    "\n",
    "- $\\text{Bias}[\\hat{f}(X)] = \\mathbb{E}[\\hat{f}(X)] - f(X)$ はバイアス（モデルの期待値と真の関数$f(X)$との差）。\n",
    "- $\\text{Var}[\\hat{f}(X)] = \\mathbb{E}[(\\hat{f}(X) - \\mathbb{E}[\\hat{f}(X)])^2]$ はバリアンス（モデルの予測のばらつき）。\n",
    "- $\\sigma^2$ はノイズ（観測値のばらつき）。\n",
    "\n",
    "### バイアスとバリアンスのトレードオフ\n",
    "\n",
    "- **バイアスが高い**：モデルが真の関数をうまく捕捉できていない状態（過少学習）。例えば、線形回帰モデルが非線形データをフィットする場合。\n",
    "- **バリアンスが高い**：モデルがデータに対して過剰にフィットしている状態（過学習）。例えば、高次の多項式回帰モデルが少数のデータポイントをフィットする場合。\n",
    "\n",
    "#### 適切なバランス\n",
    "\n",
    "理想的なモデルは、バイアスとバリアンスのバランスが取れた状態にあります。過少学習を避けるためにバイアスを低くし、過学習を避けるためにバリアンスを低く保つことが求められます。\n",
    "\n",
    "### 図解\n",
    "\n",
    "以下に、バイアスとバリアンスのトレードオフを示す図をPythonで描画する例を示します。\n",
    "\n",
    "\n",
    "\n",
    "このコードは、モデルの複雑さに応じたバイアスとバリアンスの変化を視覚的に示しています。一般に、モデルの複雑さが増すとバイアスは減少し、バリアンスは増加します。最適なモデルの複雑さは、これらのトレードオフを考慮して決定されます。\n",
    "\n",
    "### まとめ\n",
    "\n",
    "バイアス・バリアンス分解は、モデルの予測誤差をバイアス、バリアンス、およびノイズの3つの要素に分解することで、モデルの性能を評価し、過学習や過少学習の特性を理解するための重要な概念です。この分解を用いることで、モデルの選択やチューニングにおいて、バイアスとバリアンスのバランスを考慮することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# バイアスとバリアンスの関係を示すためのデータ生成\n",
    "model_complexity = np.arange(1, 11)\n",
    "bias2 = (model_complexity - 6)**2\n",
    "variance = model_complexity**2 / 10\n",
    "error = bias2 + variance + 5  # ノイズ成分を含めた誤差\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(model_complexity, bias2, label='Bias^2', marker='o')\n",
    "plt.plot(model_complexity, variance, label='Variance', marker='o')\n",
    "plt.plot(model_complexity, error, label='Total Error', marker='o')\n",
    "plt.xlabel('Model Complexity')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Bias-Variance Tradeoff')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
