{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サポートベクターマシン (SVM) について\n",
    "\n",
    "サポートベクターマシン（Support Vector Machine, SVM）は、機械学習における教師あり学習アルゴリズムの一つで、特に分類問題や回帰問題に用いられます。SVMは、データを分類するための最適な境界（ハイパープレーン）を見つけることを目的としています。\n",
    "\n",
    "### 基本的な考え方\n",
    "\n",
    "SVMの目的は、異なるクラスのデータ点を分離するためのハイパープレーンを見つけることです。このハイパープレーンは、データセット内の最も近いデータ点（サポートベクター）との距離を最大化するように選ばれます。\n",
    "\n",
    "### マージンとハイパープレーン\n",
    "\n",
    "1. **ハイパープレーン**：$n$次元のデータ空間において、$n-1$次元の平面です。例えば、2次元空間では直線、3次元空間では平面になります。\n",
    "2. **マージン**：ハイパープレーンから最も近いデータ点までの距離です。SVMは、このマージンを最大化するハイパープレーンを見つけます。\n",
    "\n",
    "### 数式による定義\n",
    "\n",
    "データ点 $\\mathbf{x}_i \\in \\mathbb{R}^n$ とそのクラスラベル $y_i \\in \\{-1, 1\\}$ が与えられたとき、ハイパープレーンは次のように定義されます：\n",
    "$$\n",
    "\\mathbf{w} \\cdot \\mathbf{x} - b = 0\n",
    "$$\n",
    "ここで、$\\mathbf{w}$ は重みベクトル、$b$ はバイアス項です。\n",
    "\n",
    "SVMは以下の最適化問題を解きます：\n",
    "$$\n",
    "\\min_{\\mathbf{w}, b} \\frac{1}{2} \\|\\mathbf{w}\\|^2\n",
    "$$\n",
    "ただし、次の制約を満たします：\n",
    "$$\n",
    "y_i (\\mathbf{w} \\cdot \\mathbf{x}_i - b) \\geq 1, \\quad \\forall i\n",
    "$$\n",
    "\n",
    "### ソフトマージン SVM\n",
    "\n",
    "データが完全に線形分離できない場合、ソフトマージン SVM を使用します。ソフトマージン SVM では、制約に違反するデータ点に対してペナルティを課します。ペナルティの度合いは、ハイパーパラメータ $C$ によって制御されます。最適化問題は次のように修正されます：\n",
    "$$\n",
    "\\min_{\\mathbf{w}, b, \\xi} \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^N \\xi_i\n",
    "$$\n",
    "ただし、次の制約を満たします：\n",
    "$$\n",
    "y_i (\\mathbf{w} \\cdot \\mathbf{x}_i - b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0, \\quad \\forall i\n",
    "$$\n",
    "ここで、$\\xi_i$ はスラック変数で、データ点がマージンの内側または反対側に位置する度合いを表します。\n",
    "\n",
    "### カーネル SVM\n",
    "\n",
    "線形分離できないデータに対しては、カーネル関数を用いることができます。カーネル関数を使うことで、データを高次元空間にマッピングし、その空間で線形分離を行います。よく使われるカーネルには次のようなものがあります：\n",
    "- 線形カーネル：$K(\\mathbf{x}, \\mathbf{x}') = \\mathbf{x} \\cdot \\mathbf{x}'$\n",
    "- 多項式カーネル：$K(\\mathbf{x}, \\mathbf{x}') = (\\mathbf{x} \\cdot \\mathbf{x}' + c)^d$\n",
    "- ガウシアンRBFカーネル：$K(\\mathbf{x}, \\mathbf{x}') = \\exp(-\\gamma \\|\\mathbf{x} - \\mathbf{x}'\\|^2)$\n",
    "\n",
    "### 実装例\n",
    "\n",
    "以下に、Pythonとscikit-learnを用いてSVMを実装する例を示します。ここでは、Irisデータセットを使用してSVMを訓練し、結果を可視化します。\n",
    "\n",
    "\n",
    "このコードでは、Irisデータセットを用いて線形カーネルSVMを訓練し、各クラスのデータ点とサポートベクターをプロットしています。プロットから、サポートベクターがデータ点の境界に位置していることがわかります。\n",
    "\n",
    "### まとめ\n",
    "\n",
    "サポートベクターマシン（SVM）は、データの分離を最大化するハイパープレーンを見つける強力な手法です。線形分離が難しい場合でも、カーネルトリックを用いることで高次元空間での分離を実現できます。SVMは分類問題において高い性能を発揮することが多く、機械学習の多くの応用分野で広く利用されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Irisデータセットの読み込み\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # 特徴量として最初の2つの特徴を使用\n",
    "y = iris.target\n",
    "\n",
    "# データの分割（訓練データとテストデータ）\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# SVMの訓練\n",
    "model = SVC(kernel='linear', C=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# サポートベクターの取得\n",
    "support_vectors = model.support_vectors_\n",
    "\n",
    "# プロット\n",
    "plt.figure()\n",
    "colors = ['red', 'green', 'blue']\n",
    "target_names = iris.target_names\n",
    "for color, i, target_name in zip(colors, [0, 1, 2], target_names):\n",
    "    plt.scatter(X[y == i, 0], X[y == i, 1], alpha=0.8, color=color, label=target_name)\n",
    "plt.scatter(support_vectors[:, 0], support_vectors[:, 1], s=100, facecolors='none', edgecolors='k', label='Support Vectors')\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('SVM on IRIS dataset with linear kernel')\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
